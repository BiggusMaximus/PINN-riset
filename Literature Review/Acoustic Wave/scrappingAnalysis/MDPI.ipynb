{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from csv import DictWriter\n",
    "from threading import Thread\n",
    "import os, csv, re, queue, time, winsound, traceback, selenium\n",
    "import pycountry\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--ignore-ssl-errors')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--incognito')\n",
    "chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "caps = webdriver.DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"normal\"\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "LINK = 'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&field1=AllField&text1=LEACH+AND+WSN+AND+ENERGY&pageSize=50&expand=all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "link ='https://www.mdpi.com/search?sort=pubdate&page_count=200&year_from=1996&year_to=2024&advanced=(%40(title%2Cabstract%2Ckeywords%2Cauthors%2Caffiliations%2Cdoi%2Cfull_text%2Creferences)LEACH%40(title%2Cabstract%2Ckeywords%2Cauthors%2Caffiliations%2Cdoi%2Cfull_text%2Creferences)energy%40(title%2Cabstract%2Ckeywords%2Cauthors%2Caffiliations%2Cdoi%2Cfull_text%2Creferences)WSN)&view=abstract'\n",
    "\n",
    "driver.get(link)\n",
    "allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    ")\n",
    "\n",
    "# Click the 'Allow all cookies' button\n",
    "allow_all_cookies_button.click()\n",
    "html = driver.page_source\n",
    "driver.close()\n",
    "soups = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:\n",
      "\t\t\t\t0\n",
      "link:\n",
      "\t\t\t\t0\n",
      "number_of_citation:\n",
      "\t\t\t\t0\n",
      "article_type:\n",
      "\t\t\t\t0\n",
      "publisher:\n",
      "\t\t\t\tACM\n",
      "keyword:\n",
      "\t\t\t\t0\n",
      "abstract:\n",
      "\t\t\t\t0\n",
      "publish_date:\n",
      "\t\t\t\t0\n",
      "publication_title:\n",
      "\t\t\t\t0\n",
      "authors:\n",
      "\t\t\t\t0\n",
      "affiliations:\n",
      "\t\t\t\t0\n",
      "countries:\n",
      "\t\t\t\t0\n"
     ]
    }
   ],
   "source": [
    "row_data = {\n",
    "        'title'                         : '0',\n",
    "        'link'                          : '0',\n",
    "        'number_of_citation'            : '0',\n",
    "        'article_type'                  : '0',\n",
    "        'publisher'                     : 'ACM',\n",
    "        'keyword'                       : '0',\n",
    "        'abstract'                      : '0',\n",
    "        'publish_date'                  : '0',\n",
    "        'publication_title'             : '0',\n",
    "        'authors'                       : '0',\n",
    "        'affiliations'                  : '0',\n",
    "        'countries'                     : '0'\n",
    "    }\n",
    "    \n",
    "for col in row_data.keys():\n",
    "    print(f\"{col}:\\n\\t\\t\\t\\t{row_data[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m soups\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneric-item article-item\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m5\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice, options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[1;32m----> 4\u001b[0m     link_article   \u001b[38;5;241m=\u001b[39m \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolor-grey-dark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m()\n\u001b[0;32m      5\u001b[0m     link_article   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://doi.org/10.3390/s24206502\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m     article_type   \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel articletype\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "for article in soups.find_all('div', class_='generic-item article-item')[:5]:\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    link_article   = article.find('div', class_='color-grey-dark').find('a').get_text()\n",
    "    link_article   = 'https://doi.org/10.3390/s24206502'\n",
    "    article_type   = article.find('span', class_='label articletype').get_text()\n",
    "    article_title  = article.find('a', class_='title-link').get_text()\n",
    "    driver.get(link_article)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # allow_all_cookies_button = None\n",
    "    # # Click on 'Info' section with error handling\n",
    "    # try:\n",
    "    #     allow_all_cookies_button = driver.find_element(By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\")\n",
    "        \n",
    "    #     max_attempts = 3\n",
    "    #     for attempt in range(max_attempts):\n",
    "    #         try:\n",
    "    #             allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "    #                 EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "    #             )\n",
    "    #             allow_all_cookies_button.click()\n",
    "    #             break\n",
    "    #         except selenium.common.exceptions.ElementClickInterceptedException:\n",
    "    #             if attempt < max_attempts - 1:\n",
    "    #                 print(\"Retrying click after a short delay...\")\n",
    "    #                 time.sleep(1)\n",
    "    #             else:\n",
    "    #                 print(\"Failed to click after multiple attempts.\")\n",
    "    #                 raise\n",
    "\n",
    "    # except selenium.common.exceptions.TimeoutException:\n",
    "    #     print(\"Element was not clickable within the given time.\")\n",
    "    #     continue\n",
    "    # allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "    #     EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "    # )\n",
    "    # allow_all_cookies_button.click()\n",
    "    html = driver.page_source\n",
    "    article_soup = BeautifulSoup(html, 'html.parser')\n",
    "    publication_title = article_soup.find('div', class_='bib-identity').find('em').get_text()\n",
    "\n",
    "    number_of_citation = article_soup.find('div', id='counts-wrapper').find('span', class_='count citations-number Var_ArticleMaxCitations')\n",
    "\n",
    "    if number_of_citation:\n",
    "        number_of_citation = number_of_citation.get_text()\n",
    "    else:\n",
    "        number_of_citation = 'none'\n",
    "   \n",
    "    keywords = article_soup.find_all('div', class_='html-gwd-group')\n",
    "    if keywords != None:\n",
    "        for keyword in keywords:\n",
    "            keyword = keyword.find_all('a')\n",
    "            keyword = ','.join([i.get_text() for i in  keyword])\n",
    "        keywords = keyword\n",
    "    else:\n",
    "        keywords = 'none'\n",
    "\n",
    "    abstract = article_soup.find('section', class_=\"html-abstract\") \n",
    "    if abstract != None:\n",
    "        abstract = abstract.get_text()\n",
    "    else:\n",
    "        abstract = 'none'\n",
    "\n",
    "    publication_date = article_soup.find('div', class_='pubhistory').find_all('span')\n",
    "    if len(publication_date) == 3:\n",
    "        publication_date = publication_date[2].get_text()\n",
    "        publication_date = publication_date.replace('Accepted: ', '')\n",
    "    elif len(publication_date) == 4:\n",
    "        publication_date = publication_date[2].get_text()\n",
    "        publication_date = publication_date.replace('Published: ', '')\n",
    "    \n",
    "    publication_title = article_soup.find('div', class_='bib-identity').find('em').get_text()\n",
    "    \n",
    "    affiliation_info = {}\n",
    "    for i in article_soup.find_all('div', class_=\"affiliation\"):\n",
    "        affiliation_info[i.find('sup').get_text()] = i.find('div', class_='affiliation-name').get_text()\n",
    "\n",
    "\n",
    "    affiliation_info.pop('*')\n",
    "    author_informations = article_soup.find('div', class_='art-authors hypothesis_container')\n",
    "    author_informations = author_informations.find_all('span', class_=\"inlineblock\")\n",
    "\n",
    "    authors = ''\n",
    "    countries = ''\n",
    "    affiliations = ''\n",
    "    count = 0\n",
    "\n",
    "    for author_information in author_informations:\n",
    "        author = author_information.find('span', class_='sciprofiles-link__name').get_text()\n",
    "        index_affiliations = author_information.find('sup').get_text().split(',')\n",
    "        for index_affiliation in index_affiliations:\n",
    "            if index_affiliation != '*':\n",
    "                index_affiliation = index_affiliation.replace(\" \", \"\")\n",
    "                affiliation = affiliation_info[index_affiliation]\n",
    "                country = affiliation.split(',')\n",
    "                countries += country[len(country)-1]\n",
    "\n",
    "                affiliations +=  f'({affiliation})'\n",
    "\n",
    "                affiliation = affiliation.split(', ')\n",
    "                country = affiliation[len(affiliation)-1]\n",
    "                countries       += ','\n",
    "                affiliations    += ','\n",
    "                    \n",
    "        authors += author\n",
    "        if count < len(author_informations)-1:\n",
    "            authors         += ','\n",
    "        count += 1\n",
    "        occurrences = countries.split(',')\n",
    "        found_countries = []\n",
    "\n",
    "        for country in pycountry.countries:\n",
    "            for occurrence in occurrences:\n",
    "                if country.name in occurrence:\n",
    "                    found_countries.append(country.name)\n",
    "\n",
    "        countries = ','.join(found_countries)\n",
    "\n",
    "        row_data = {\n",
    "                'title'                         : article_title,\n",
    "                'link'                          : link_article,\n",
    "                'number_of_citation'            : number_of_citation,\n",
    "                'article_type'                  : article_type,\n",
    "                'publisher'                     : 'ACM',\n",
    "                'keyword'                       : keywords,\n",
    "                'abstract'                      : abstract,\n",
    "                'publish_date'                  : publication_date,\n",
    "                'publication_title'             : publication_title,\n",
    "                'authors'                       : authors,\n",
    "                'affiliations'                  : affiliations,\n",
    "                'countries'                     : countries\n",
    "            }\n",
    "        \n",
    "        for col in row_data.keys():\n",
    "            print(f\"{col}:\\n\\t\\t\\t\\t{row_data[col]}\")\n",
    "    \n",
    "    driver.close()\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(\"\\n\"*3)\n",
    "    #     print(\"== ! Error ! ==\"*10)\n",
    "    #     print(\"\\n\"*1)\n",
    "    #     print(f\"Failed to retrieve article: \\n{article_title}\\n{link_article}\\n{e}\")\n",
    "    #     traceback.print_exc()\n",
    "    #     print(\"\\n\"*3)\n",
    "    #     print(\"== ! Error ! ==\"*10)\n",
    "    #     print(\"\\n\"*1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"color-grey-dark\">\n",
       "<em>Sensors</em> <b>2024</b>, <em>24</em>(21), 6952; https://doi.org/10.3390/s24216952 (registering DOI) - 30 Oct 2024\n",
       "    </div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.3390/s24216952\n"
     ]
    }
   ],
   "source": [
    "html = str(article.find('div', class_='color-grey-dark'))\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract the text content of the div and find the DOI link\n",
    "text = soup.get_text()\n",
    "doi_link = re.search(r'https://doi\\.org/\\S+', text).group(0)\n",
    "print(doi_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
