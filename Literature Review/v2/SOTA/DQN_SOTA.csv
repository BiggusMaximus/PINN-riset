Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Index Keywords,Molecular Sequence Numbers,Chemicals/CAS,Tradenames,Manufacturers,Funding Details,Funding Texts,References,Correspondence Address,Editors,Publisher,Sponsors,Conference name,Conference date,Conference location,Conference code,ISSN,ISBN,CODEN,PubMed ID,Language of Original Document,Abbreviated Source Title,Document Type,Publication Stage,Open Access,Source,EID,Unified Keywords,Unified Title
Ghimire M.; Zhang L.; Zhang W.; Ren Y.; Xu Z.,"Ghimire, Mukesh (57394029500); Zhang, Lei (57220902398); Zhang, Wenlong (55325002000); Ren, Yi (57843074600); Xu, Zhe (59300749200)",57394029500; 57220902398; 55325002000; 57843074600; 59300749200,Solving Two-Player General-Sum Game between Swarms,2024,Proceedings of the American Control Conference,,,,56,61,5.0,0,10.23919/ACC60939.2024.10644320,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204494381&doi=10.23919%2fACC60939.2024.10644320&partnerID=40&md5=d44fac8563451c1caada995b256cdf7c,"School of Engineering, Matter, Transport, and Energy, Arizona State University, Tempe, 85287, AZ, United States; School of Manufacturing Systems and Networks, Arizona State University, Mesa, 85212, AZ, United States","Ghimire M., School of Engineering, Matter, Transport, and Energy, Arizona State University, Tempe, 85287, AZ, United States; Zhang L., School of Engineering, Matter, Transport, and Energy, Arizona State University, Tempe, 85287, AZ, United States; Zhang W., School of Manufacturing Systems and Networks, Arizona State University, Mesa, 85212, AZ, United States; Ren Y., School of Engineering, Matter, Transport, and Energy, Arizona State University, Tempe, 85287, AZ, United States; Xu Z., School of Engineering, Matter, Transport, and Energy, Arizona State University, Tempe, 85287, AZ, United States","Hamilton-Jacobi-Isaacs (HJI) PDEs are the governing equations for the two-player general-sum games. Un-like Reinforcement Learning (RL) methods, which are data-intensive methods for learning value function, learning HJ PDEs provide a guaranteed convergence to the Nash Equilibrium value of the game when it exists. However, a caveat is that solving HJ PDEs becomes intractable when the state dimension increases. To circumvent the curse of dimensionality (CoD), physics-informed machine learning methods with supervision can be used and have been shown to be effective in generating equilibrial policies in two-player general-sum games. In this work, we extend the existing work on agent-level two-player games to a two-player swarm-level game, where two sub-swarms play a general-sum game. We consider the Kolmogorov forward equation as the dynamic model for the evolution of the densities of the swarms. Results show that policies generated from the physics-informed neural network (PINN) result in a higher payoff than a Nash Deep Q-Network (Nash DQN) agent and have comparable performance with numerical solvers. Â© 2024 AACC.",,Adversarial machine learning; Deep neural networks; Reinforcement learning; Supervised learning; Swarm intelligence; Data intensive; Equilibrium value; Function learning; General sum games; Governing equations; Guaranteed convergence; Hamilton-Jacobi-Isaacs; Nash equilibria; Reinforcement learning method; Value functions; Nash equilibrium,,,,,"National Science Foundation, NSF, (CNS 2304863, CNS 2339774); National Science Foundation, NSF; Office of Naval Research, ONR, (ONR N00014-23-1-2505); Office of Naval Research, ONR","This research is partially supported by the National Science Foundation under grants CNS 2304863 and CNS 2339774, and the Office of Naval Research under grant ONR N00014-23-1-2505.","Schranz M., Umlauft M., Sende M., Elmenreich W., Swarm robotic behaviors and current applications, Frontiers in Robotics and AI, (2020); Berman S., Halasz A., Hsieh M.A., Kumar V., Optimized stochastic policies for task allocation in swarms of robots, IEEE transactions on robotics, 25, 4, pp. 927-937, (2009); Kushleyev A., Mellinger D., Powers C., Kumar V., Towards a swarm of agile micro quadrotors, Autonomous Robots, 35, 4, pp. 287-300, (2013); Liu W., Winfield A.F., Sa J., Chen J., Dou L., Towards energy optimization: Emergent task allocation in a swarm of foraging robots, Adaptive behavior, 15, 3, pp. 289-305, (2007); Ai X., Srinivasan V., Tham C.-K., Optimality and complexity of pure nash equilibria in the coverage game, IEEE Journal on Selected Areas in Communications, 26, 7, pp. 1170-1182, (2008); Paraskevas E., Maity D., Baras J.S., Distributed energy-aware mobile sensor coverage: A game theoretic approach, 2016 Ameri-can Control Conference (ACC), pp. 6259-6264, (2016); Elamvazhuthi K., Berman S., Mean-field models in swarm robotics: A survey, Bioinspiration & Biomimetics, 15, 1, (2019); Sinigaglia C., Manzoni A., Braghin F., Density control of largescale particles swarm through pde-constrained optimization, IEEE Transactions on Robotics, 38, 6, pp. 3530-3549, (2022); Tuyls K., Tumer K., Weiss G., Multiagent learning, Multiagent Systems, pp. 423-475, (2013); Zhang L., Ghimire M., Zhang W., Xu Z., Ren Y., Approximating discontinuous nash equilibrial values of two-player general-sum differential games, 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 3022-3028, (2023); Crandall M.G., Lions P.-L., Viscosity solutions of hamiltonjacobi equations, Transactions of the American mathematical society, 277, 1, pp. 1-42, (1983); Mitchell I.M., Tomlin C.J., Overapproximating reachable sets by hamilton-jacobi projections, journal of Scientific Computing, 19, pp. 323-346, (2003); Lu Z., Pu H., Wang F., Hu Z., Wang L., The expressive power of neural networks: A view from the width, Advances in neural information processing systems, 30, (2017); Elamvazhuthi K., Kakish Z., Shirsat A., Berman S., Controllability and stabilization for herding a robotic swarm using a leader: A mean-field approach, IEEE Transactions on Robotics, 37, 2, pp. 418-432, (2021); Milutinovic D., Lima P., Modeling and optimal centralized control of a large-size robotic population, IEEE Transactions on Robotics, 22, 6, pp. 1280-1285, (2006); Djeumou F., Xu Z., Cubuktepe M., Topcu U., Probabilistic control of heterogeneous swarms subject to graph temporal logic specifications: A decentralized and scalable approach, IEEE Transactions on Automatic Control, 68, 4, pp. 2245-2260, (2022); Bansal S., Tomlin C., DeepReach: A deep learning approach to high-dimensional reachability, IEEE International Conference on Robotics and Automation (ICRA), (2021); Bertsekas D.P., Rollout algorithms for discrete optimization: A survey; Bertsekas D., Multiagent reinforcement learning: Rollout and policy iteration, IEEE/CAA Journal of Automatica Sinica, 8, 2, pp. 249-272, (2021); Hu J., Wellman M.P., Nash q-learning for general-sum stochastic games, Journal of machine learning research, 4, pp. 1039-1069, (2003); Casgrain P., Ning B., Jaimungal S., Deep q-learning for nash equilibria: Nash-dqn, Applied Mathematical Finance, 29, 1, pp. 62-78, (2022); Starr A.W., Ho Y.-C., Nonzero-sum differential games, Journal of optimization theory and applications, 3, pp. 184-206, (1969); Mnih V., Kavukcuoglu K., Silver D., Graves A., Antonoglou I., Wierstra D., Riedmiller M., Playing atari with deep reinforcement learning, (2013); Lemke C.E., Howson J.T., Equilibrium points of bimatrix games, Journal of the Society for industrial and Applied Mathematics, 12, 2, pp. 413-423, (1964); Knight V., Katiemcgoldrick, Panayides M., Wang Y., Gaba A.S., Tokheim I.F., Konovalov O., Campbell J., Glynatsi N., Riviere P., Baldevia R., Szeto R., Arwheel, newaijj, and volume-on max, (2023)","Z. Xu; School of Engineering, Matter, Transport, and Energy, Arizona State University, Tempe, 85287, United States; email: xzhe1@asu.edu",,Institute of Electrical and Electronics Engineers Inc.,Boeing; Elsevier; et al.; Halliburton; MathWorks; Mitsubishi Electric Research Laboratories (MERL),"2024 American Control Conference, ACC 2024",10 July 2024 through 12 July 2024,Toronto,202444.0,07431619,979-835038265-5,PRACE,,English,Proc Am Control Conf,Conference paper,Final,,Scopus,2-s2.0-85204494381,Adversarial machine learning; Deep neural networks; Reinforcement learning; Supervised learning; Swarm intelligence; Data intensive; Equilibrium value; Function learning; General sum games; Governing equations; Guaranteed convergence; Hamilton-Jacobi-Isaacs; Nash equilibria; Reinforcement learning method; Value functions; Nash equilibrium,Solving Two-Player General-Sum Game between Swarms
